 <!DOCTYPE html>
<html>
<head>
<title>Page Title</title>
</head>
<body>

<h1>This is a Heading</h1>
<p>This is a paragraph.</p>

<div>
  
************************* EXP 1 (data preprocessing) *********************
</div>

  <code>
    
import pandas as pd
from sklearn.impute import SimpleImputer
from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler
# Create the DataFrame
data = {
    'Student ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Exam 1 Score': [85, 76, 90, 65, 88, None, 78, 92, 85, 70],
    'Exam 2 Score': [92, 78, 88, 75, 91, 82, 76, 96, 89, 68],
    'Exam 3 Score': [88, None, 94, 80, 87, 79, 72, 98, 91, 75],
    'Final Grade': ['A', 'B', 'A', 'C', 'A', 'B', 'C', 'A', 'A', 'B']
}
df = pd.DataFrame(data)
# Imputation
imputer = SimpleImputer(strategy="mean")
df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']] = imputer.fit_transform(df[['Exam 1 Score','Exam 2 Score','Exam 3 Score']])
# Anomaly Detection
z_scores = stats.zscore(df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']])
threshold = 3
outliers = (abs(z_scores) > threshold).any(axis=1)
df['Is Outlier'] = outliers
# Standardization
scaler = StandardScaler()
df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']] = scaler.fit_transform(df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']])
# Normalization
scaler = MinMaxScaler()
df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']] = scaler.fit_transform(df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']])
# Output
print("Final Preprocessed Dataset:\n", df)

  </code>
<div>
  
************************* EXP 2 (Gradient Descent) *********************
</div>
<code>
  
import numpy as np
import matplotlib.pyplot as plt
def objective_function(x):
 return x**2
def gradient(x):
 return 2 * x
learning_rate = 0.1
initial_x = 5
num_iterations = 20
x = initial_x
x_history = [x]
loss_history = [objective_function(x)]
for i in range(num_iterations):
    grad = gradient(x)
    x -= learning_rate * grad
    x_history.append(x)
    loss_history.append(objective_function(x))
    
x_values = np.linspace(-6, 6, 400)
y_values = objective_function(x_values)
plt.plot(x_values, y_values, label='Objective Function')
plt.scatter(x_history, loss_history, color='red', label='Gradient Descent Steps')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Gradient Descent Optimization')
plt.legend()
plt.show()
</code>
<div>
  
************************* EXP 3 (Simple Linear) *********************
</div>
<code>
  
import numpy as np
import matplotlib.pyplot as plt


class SimpleLinearRegression:
    def _init_(self, learning_rate=0.01, num_iterations=1000):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.slope = None
        self.intercept = None
    def fit(self, X, y):
        num_samples, num_features = X.shape
        self.slope = np.zeros(num_features)
        self.intercept = 0
        
        for _ in range(self.num_iterations):
            y_pred = np.dot(X, self.slope) + self.intercept
            grad_slope = -(2 / num_samples) * np.dot(X.T, y - y_pred)
            grad_intercept = -(2 / num_samples) * np.sum(y - y_pred)

            self.slope -= self.learning_rate * grad_slope
            self.intercept -= self.learning_rate * grad_intercept
    def predict(self, X):
        return np.dot(X, self.slope) + self.intercept


# Sample data
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 5])
model = SimpleLinearRegression(learning_rate=0.01, num_iterations=1000)
model.fit(X, y)
y_pred = model.predict(X)
plt.scatter(X, y, label="Actual data")
plt.plot(X, y_pred, color='red', label="Fitted line")
plt.xlabel("X")
plt.ylabel("y")
plt.title("Simple Linear Regression")
plt.legend()
plt.show()
# Print the slope and intercept
print("Slope:", model.slope[0])
print("Intercept:", model.intercept)

</code>
<div>
  
************************* EXP 4 (Logistic) *********************
</div>
<code>
  
import numpy as np
import matplotlib.pyplot as plt

class LogisticRegression:
    def _init_(self, learning_rate=0.01, num_iterations=1000):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.weights = None
        self.bias = None
        
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y):
        num_samples, num_features = X.shape
        self.weights = np.zeros(num_features)
        self.bias = 0
        
        for _ in range(self.num_iterations):
            linear_model = np.dot(X, self.weights) + self.bias
            predictions = self.sigmoid(linear_model)
            dw = (1 / num_samples) * np.dot(X.T, (predictions - y))
            db = (1 / num_samples) * np.sum(predictions - y)
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
    def predict(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        predictions = self.sigmoid(linear_model)
        predicted_labels = [1 if p >= 0.5 else 0 for p in predictions]
        return predicted_labels


# Sample data
X = np.array([[2.5, 1.5], [3.0, 1.0], [4.0, 3.0], [1.0, 4.0], [2.0, 2.0]])
y = np.array([1, 1, 0, 0, 1])
model = LogisticRegression(learning_rate=0.01, num_iterations=1000)
model.fit(X, y)
y_pred = model.predict(X)
# Print the learned weights and bias
print("Learned Weights:", model.weights)
print("Learned Bias:", model.bias)
print("Predicted Labels:", y_pred)


</code>
  <div>
    
************************* EXP 5 () ************************
  </div>
<code>
  
from sklearn.datasets import make_classification
from sklearn import tree
from sklearn.model_selection import train_test_split
X, t = make_classification(100, 5, n_classes=2, shuffle=True, random_state=10)
X_train, X_test, t_train, t_test = train_test_split(
    X, t, test_size=0.3, shuffle=True, random_state=1)
model = tree.DecisionTreeClassifier()
model = model.fit(X_train, t_train)
predicted_value = model.predict(X_test)
print(predicted_value)
tree.plot_tree(model)
zeroes = 0
ones = 0
for i in range(0, len(t_train)):
    if t_train[i] == 0:
        zeroes += 1
    else:
        ones += 1
print(zeroes)
print(ones)
val = 1 - ((zeroes/70)(zeroes/70) + (ones/70)(ones/70))
print("Gini :", val)
match = 0
UnMatch = 0
for i in range(30):
    if predicted_value[i] == t_test[i]:
        match += 1
    else:
        UnMatch += 1
accuracy = match/30
print("Accuracy is: ", accuracy)

</code>
<div>
  
************************* EXP 6 (SVM) ************************
</div>
<code>
  
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

cancer = datasets.load_breast_cancer()
X = cancer.data
y = cancer.target
print(X)
print("\n")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state = 42)
svm_classifier = SVC(kernel = 'linear', C = 1.0)

svm_classifier.fit(X_train, y_train)
y_pred = svm_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print("\nClassification Report:\n", classification_report(y_test,
y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

</code>
<div>
  
************************* EXP 7 (Ensemble) ************************
</div>
<code>
  
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import VotingClassifier

# Load the dataset as an example
data = load_breast_cancer()
X, y = data.data, data.target
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)

# BAGGING (BootStrap Aggregating)

rf_classifier = RandomForestClassifier(n_estimators=100,random_state=42)
rf_classifier.fit(X_train, y_train)
rf_predictions = rf_classifier.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print("Predictions:", rf_predictions)
print(f"Accuracy: {rf_accuracy}\n")

# BOOSTING
# AdaBoost
adaboost_classifier = AdaBoostClassifier(n_estimators=100,
random_state=42)
adaboost_classifier.fit(X_train, y_train)
adaboost_predictions = adaboost_classifier.predict(X_test)
adaboost_accuracy = accuracy_score(y_test, adaboost_predictions)
print("Predictions:", adaboost_predictions)
print(f"Accuracy: {adaboost_accuracy}\n")
# Gradient Boost
gradientboost_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)
gradientboost_classifier.fit(X_train, y_train)
gradientboost_predictions = gradientboost_classifier.predict(X_test)
gradientboost_accuracy = accuracy_score(y_test,
gradientboost_predictions)
print("Predictions:", gradientboost_predictions)
print(f"Accuracy: {gradientboost_accuracy}\n")

# STACKING
# Stacking involves training multiple diverse models and then
# Create base models
base_models = [
('RandomForest', RandomForestClassifier(n_estimators=100,
random_state=42)),
('AdaBoost', AdaBoostClassifier(n_estimators=100,
random_state=42)),
('GradientBoosting',
GradientBoostingClassifier(n_estimators=100, random_state=42))
]
# Create a meta-classifier (Logistic Regression)
meta_classifier = LogisticRegression()
# Create the stacking ensemble
stacking_classifier = VotingClassifier(estimators=base_models,
voting='soft', n_jobs=-1)
stacking_classifier.fit(X_train, y_train)
stacking_predictions = stacking_classifier.predict(X_test)
stacking_accuracy = accuracy_score(y_test, stacking_predictions)
print("Predictions:", stacking_predictions)
print(f"Stacking Accuracy: {stacking_accuracy}")


</code>

<div>
  
************************* EXP 8 (PCA) ************************
</div>
<code>
  
import numpy as np
import matplotlib.pyplot as plt

# Create sample data
np.random.seed(0)
data = np.random.randn(2, 50)
data[1, :] = 2 * data[0, :] + 1 + 0.5 * np.random.randn(50)

# Standardize the data (mean-centering)
mean_centered_data = data - np.mean(data, axis=1).reshape(-1, 1)

# Compute the covariance matrix
cov_matrix = np.cov(mean_centered_data)

# Calculate eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Sort eigenvalues and eigenvectors
eigenvalue_index = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[eigenvalue_index]
eigenvectors = eigenvectors[:, eigenvalue_index]

# Choose the number of principal components
num_components = 1

# Project the data onto the principal component(s)
projected_data = np.dot(eigenvectors[:, :num_components].T, mean_centered_data)

# Display the covariance matrix
print("Covariance Matrix:")
print(cov_matrix)

# Display the eigenvalues
print("\nEigenvalues:")
print(eigenvalues)

# Display the eigenvectors
print("\nEigenvectors:")
print(eigenvectors)

# Display the first principal component
first_principal_component = eigenvectors[:, 0]
print("\nFirst Principal Component:")
print(first_principal_component)

# Plot the original data
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.scatter(data[0, :], data[1, :])
plt.title("Original Data")
plt.subplot(1, 3, 2)
plt.scatter(projected_data, np.zeros_like(projected_data))
plt.title("Data on First Principal Component")
plt.subplot(1, 3, 3)
plt.quiver(0, 0, first_principal_component[0], first_principal_component[1], angles='xy')
plt.title("First Principal Component")
</code>

 <div>
  
*********************PIUSH PAUL***************************
 </div>

 
 Preprocessing 

 import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder

# Load data from CSV
data = pd.read_csv('preprocessing.csv')

# Display the original data
print("Original Data:")
print(data)
print()

# 1. Imputation using mean for missing values
data_imputed = data.fillna(data.mean())

print("Imputed Data:")
print(data_imputed)
print()

# 2. Anomaly detection (here using a simple threshold-based approach)
threshold = 2  # Set the anomaly threshold
anomaly_mask = (data_imputed - data_imputed.mean()).abs() > threshold * data_imputed.std()
anomaly_indices = anomaly_mask.any(axis=1)

print("Anomaly Indices:")
print(anomaly_indices)
print()

# 3. Standardization
scaler = StandardScaler()
data_standardized = scaler.fit_transform(data_imputed[['A', 'B', 'C']])

print("Standardized Data:")
print(data_standardized)
print()

# 4. Normalization
min_max_scaler = MinMaxScaler()
data_normalized = min_max_scaler.fit_transform(data_imputed[['A', 'B', 'C']])

print("Normalized Data:")
print(data_normalized)
print()

# 5. Encoding of categorical data
label_encoder = LabelEncoder()
data_encoded = label_encoder.fit_transform(data_imputed['Category'])

print("Encoded Category:")
print(data_encoded)

 2. Gradient descent 

 import numpy as np
import matplotlib.pyplot as plt

def objective(x):
    return x**2.0

def derivative(x):
    h = 1e-6
    return (objective(x + h) - objective(x - h)) / (2*h)

def gradient_descent(objective, derivative, bounds, n_iter, step_size):
    solutions, scores = [], []
    solution = bounds[:, 0] + np.random.rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])

    for _ in range(n_iter):
        gradient = derivative(solution)
        solution = solution - step_size * gradient
        solution_eval = objective(solution)
        solutions.append(solution)
        scores.append(solution_eval)

    return solutions, scores

bounds = np.array([[-1.0, 1.0]])
n_iter = 200
step_size = 0.01
solutions, scores = gradient_descent(objective, derivative, bounds, n_iter, step_size)

inputs = np.arange(bounds[0, 0], bounds[0, 1] + 0.1, 0.1)
results = objective(inputs)

plt.plot(inputs, results)
plt.plot(solutions, scores, '.-', color='red')
plt.show()

3. Regression 

 #https://www.kaggle.com/code/mtalhazafar/sales-forecasting-by-linear-regression/notebook
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

df = pd.read_csv("advertising.csv")
print(df)

df.info()
df.describe()
df.isnull().sum()

X = df[['TV', 'Newspaper', 'Radio']] #Multiple
# X = df[['TV']] # Linear
y = df['Sales']
X.head()
y.head()

X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=100)

model = LinearRegression()
model.fit(X_train , y_train )
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("R-squared:", r2)

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Actual Values (y_test)")
plt.ylabel("Predicted Values (y_pred)")
plt.title("Actual vs. Predicted Values")
regression_line_x = np.linspace(min(y_test), max(y_test), 100)
regression_line_y = regression_line_x
plt.plot(regression_line_x, regression_line_y, color='red', linestyle=':', label="Regression Line")
plt.legend()
plt.show()


 4. Logistics regression 

 #https://www.kaggle.com/code/amolambkar/iris-data-logistic-regression
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

data = pd.read_csv('Iris.csv')
print(data.head()) #print first 5 rows of dataset

print(data.columns)
data.info()



data.drop('Id',axis=1,inplace=True)



train , test = train_test_split(data,test_size=0.2,random_state=0)

print('shape of training data : ',train.shape)
print('shape of testing data',test.shape)

train_x = train.drop(columns=['Species'],axis=1)
train_y = train['Species']

test_x = test.drop(columns=['Species'],axis=1)
test_y = test['Species']

model = LogisticRegression(solver='liblinear')
model.fit(train_x,train_y)

predict = model.predict(test_x)

print('Predicted Values on Test Data',predict)

print('\n\nAccuracy Score on test data : \n\n')
print(accuracy_score(test_y,predict))

5. Decision tree 

 import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
%matplotlib inline

data = 'car_evaluation.csv'
df = pd.read_csv(data, header=None)

# Rename the column
col_names = ['buying', 'meant', 'doors', 'persons', 'lug_boot', 'safety', 'class']

df.columns = col_names
df.head()
df.info()

for col in col_names:
    print(df[col].value_counts())

df['class'].value_counts()

df.isnull().sum()

X = df.drop(['class'], axis=1)

y = df['class']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33,random_state=42)

X_train.shape, X_test.shape

# Encode Categorical
# !pip install category_encoders
import category_encoders as ce
encoder = ce.OrdinalEncoder(cols=['buying', 'meant', 'doors', 'persons', 'lug_boot', 'safety'])

# label_encoder = LabelEncoder()
# data_encoded = label_encoder.fit_transform(X_train)


X_train = encoder.fit_transform(X_train)
X_test = encoder.transform(X_test)

from sklearn.tree import DecisionTreeClassifier

clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)
clf_gini.fit(X_train, y_train)

y_pred_gini = clf_gini.predict(X_test)
y_pred_gini[:5]

# Check Accuracy score
from sklearn.metrics import accuracy_score

print("Model Accuracy score with criterion gini index {0:0.4f}"
      .format(accuracy_score(y_pred_gini, y_test)))

y_pred_train_gini = clf_gini.predict(X_train)
y_pred_train_gini

print("Model Accuracy score with criterion gini index {0:0.4f}"
      .format(accuracy_score(y_pred_train_gini, y_train)))

print("Model Accuracy score with criterion gini index for test dataset {0:0.4f}"
      .format(accuracy_score(y_pred_gini, y_test)))
print("Model Accuracy score with criterion gini index for train dataset {0:0.4f}"
      .format(accuracy_score(y_pred_train_gini, y_train)))

plt.figure(figsize=(10, 8))
from sklearn import tree
tree.plot_tree(clf_gini.fit(X_train, y_train))


import graphviz
dot_data = tree.export_graphviz(clf_gini, out_file=None,
                              feature_names=X_train.columns,
                              class_names=y_train,
                              filled=True, rounded=True,
                              special_characters=True)

graph = graphviz.Source(dot_data)

graph

6. SVM 

 #https://www.kaggle.com/code/funxexcel/p1-sklearn-svm-model

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

from sklearn.metrics import classification_report

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data[:, :2]  # Using only the first two features for visualization
y = iris.target

# Initialize and fit the SVM model (using a linear kernel for simplicity)
svm = SVC(kernel='linear')
svm.fit(X, y)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Make predictions on the test set
y_pred = svm.predict(X_test)

# Calculate precision, recall, and F1-score for each class
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))


# Plotting the decision boundary
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
h = .02  # Step size in the mesh

xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])

Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(6, 5))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot the training points
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

plt.show()


7. Ensemble 

 import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import StackingRegressor, RandomForestRegressor
from sklearn.linear_model import Lasso, Ridge
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('advertising.csv')

# Fill missing values with mean
df.fillna(df.mean(), inplace=True)

# Split into features and target variable
X = df.drop(columns=['Sales'])
y = df['Sales']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Stacking
base_models = [('lasso', Lasso()), ('ridge', Ridge()), ('knn', KNeighborsRegressor()), ('svr', SVR()), ('dt', DecisionTreeRegressor())]
meta_model = Ridge()
stacking_regressor = StackingRegressor(estimators=base_models, final_estimator=meta_model)
stacking_regressor.fit(X_train, y_train)
y_train_pred_stack = stacking_regressor.predict(X_train)
y_test_pred_stack = stacking_regressor.predict(X_test)

# Bagging - Random Forest
random_forest_regressor = RandomForestRegressor(n_estimators=10, random_state=42)
random_forest_regressor.fit(X_train, y_train)
y_train_pred_rf = random_forest_regressor.predict(X_train)
y_test_pred_rf = random_forest_regressor.predict(X_test)

# Boosting - XGBoost
xgb_regressor = xgb.XGBRegressor(n_estimators=10, random_state=42)
xgb_regressor.fit(X_train, y_train)
y_train_pred_xgb = xgb_regressor.predict(X_train)
y_test_pred_xgb = xgb_regressor.predict(X_test)

# Evaluation Metrics - RMSE for train and test sets
rmse_stacking_train = np.sqrt(mean_squared_error(y_train, y_train_pred_stack))
rmse_stacking_test = np.sqrt(mean_squared_error(y_test, y_test_pred_stack))

rmse_rf_train = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))
rmse_rf_test = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))

rmse_boosting_train = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))
rmse_boosting_test = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))

# Create bar plots to compare RMSE of different ensemble methods on both training and test sets
labels = ['Stacking', 'Random Forest', 'XGBoost']
rmse_train = [rmse_stacking_train, rmse_rf_train, rmse_boosting_train]
rmse_test = [rmse_stacking_test, rmse_rf_test, rmse_boosting_test]

x = np.arange(len(labels))
width = 0.35

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, rmse_train, width, label='Train')
rects2 = ax.bar(x + width/2, rmse_test, width, label='Test')

ax.set_ylabel('RMSE')
ax.set_title('RMSE by Ensemble Methods')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()

plt.show()

 8. PCA

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.decomposition import PCA

# Load the breast cancer dataset
cancer = load_breast_cancer(as_frame=True)
df = cancer.frame

print(df)

# Input features
X = df[cancer.feature_names]

# Standardization
X_mean = X.mean()
X_std = X.std()
Z = (X - X_mean) / X_std

# Specify the number of principal components
n_components = 2

# Perform PCA
pca = PCA(n_components=n_components)
x_pca = pca.fit_transform(Z)

# Create a new dataframe for the principal components
df_pca = pd.DataFrame(x_pca, columns=['PC{}'.format(i + 1) for i in range(n_components)])

# Scatter plot of the first two principal components
plt.figure(figsize=(8, 6))
plt.scatter(x_pca[:, 0], x_pca[:, 1], c=cancer.target, cmap='plasma')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.show()


</body>
</html>
