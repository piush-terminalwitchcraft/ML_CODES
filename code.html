 <!DOCTYPE html>
<html>
<head>
<title>Page Title</title>
</head>
<body>

<h1>This is a Heading</h1>
<p>This is a paragraph.</p>

<div>
  
************************* EXP 1 (data preprocessing) *********************
</div>

  <code>
    
import pandas as pd
from sklearn.impute import SimpleImputer
from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler
# Create the DataFrame
data = {
    'Student ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Exam 1 Score': [85, 76, 90, 65, 88, None, 78, 92, 85, 70],
    'Exam 2 Score': [92, 78, 88, 75, 91, 82, 76, 96, 89, 68],
    'Exam 3 Score': [88, None, 94, 80, 87, 79, 72, 98, 91, 75],
    'Final Grade': ['A', 'B', 'A', 'C', 'A', 'B', 'C', 'A', 'A', 'B']
}
df = pd.DataFrame(data)
# Imputation
imputer = SimpleImputer(strategy="mean")
df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']] = imputer.fit_transform(df[['Exam 1 Score','Exam 2 Score','Exam 3 Score']])
# Anomaly Detection
z_scores = stats.zscore(df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']])
threshold = 3
outliers = (abs(z_scores) > threshold).any(axis=1)
df['Is Outlier'] = outliers
# Standardization
scaler = StandardScaler()
df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']] = scaler.fit_transform(df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']])
# Normalization
scaler = MinMaxScaler()
df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']] = scaler.fit_transform(df[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']])
# Output
print("Final Preprocessed Dataset:\n", df)

  </code>
<div>
  
************************* EXP 2 (Gradient Descent) *********************
</div>
<code>
  
import numpy as np
import matplotlib.pyplot as plt
def objective_function(x):
 return x**2
def gradient(x):
 return 2 * x
learning_rate = 0.1
initial_x = 5
num_iterations = 20
x = initial_x
x_history = [x]
loss_history = [objective_function(x)]
for i in range(num_iterations):
    grad = gradient(x)
    x -= learning_rate * grad
    x_history.append(x)
    loss_history.append(objective_function(x))
    
x_values = np.linspace(-6, 6, 400)
y_values = objective_function(x_values)
plt.plot(x_values, y_values, label='Objective Function')
plt.scatter(x_history, loss_history, color='red', label='Gradient Descent Steps')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Gradient Descent Optimization')
plt.legend()
plt.show()
</code>
<div>
  
************************* EXP 3 (Simple Linear) *********************
</div>
<code>
  
import numpy as np
import matplotlib.pyplot as plt


class SimpleLinearRegression:
    def _init_(self, learning_rate=0.01, num_iterations=1000):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.slope = None
        self.intercept = None
    def fit(self, X, y):
        num_samples, num_features = X.shape
        self.slope = np.zeros(num_features)
        self.intercept = 0
        
        for _ in range(self.num_iterations):
            y_pred = np.dot(X, self.slope) + self.intercept
            grad_slope = -(2 / num_samples) * np.dot(X.T, y - y_pred)
            grad_intercept = -(2 / num_samples) * np.sum(y - y_pred)

            self.slope -= self.learning_rate * grad_slope
            self.intercept -= self.learning_rate * grad_intercept
    def predict(self, X):
        return np.dot(X, self.slope) + self.intercept


# Sample data
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 5])
model = SimpleLinearRegression(learning_rate=0.01, num_iterations=1000)
model.fit(X, y)
y_pred = model.predict(X)
plt.scatter(X, y, label="Actual data")
plt.plot(X, y_pred, color='red', label="Fitted line")
plt.xlabel("X")
plt.ylabel("y")
plt.title("Simple Linear Regression")
plt.legend()
plt.show()
# Print the slope and intercept
print("Slope:", model.slope[0])
print("Intercept:", model.intercept)

</code>
<div>
  
************************* EXP 4 (Logistic) *********************
</div>
<code>
  
import numpy as np
import matplotlib.pyplot as plt

class LogisticRegression:
    def _init_(self, learning_rate=0.01, num_iterations=1000):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.weights = None
        self.bias = None
        
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y):
        num_samples, num_features = X.shape
        self.weights = np.zeros(num_features)
        self.bias = 0
        
        for _ in range(self.num_iterations):
            linear_model = np.dot(X, self.weights) + self.bias
            predictions = self.sigmoid(linear_model)
            dw = (1 / num_samples) * np.dot(X.T, (predictions - y))
            db = (1 / num_samples) * np.sum(predictions - y)
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
    def predict(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        predictions = self.sigmoid(linear_model)
        predicted_labels = [1 if p >= 0.5 else 0 for p in predictions]
        return predicted_labels


# Sample data
X = np.array([[2.5, 1.5], [3.0, 1.0], [4.0, 3.0], [1.0, 4.0], [2.0, 2.0]])
y = np.array([1, 1, 0, 0, 1])
model = LogisticRegression(learning_rate=0.01, num_iterations=1000)
model.fit(X, y)
y_pred = model.predict(X)
# Print the learned weights and bias
print("Learned Weights:", model.weights)
print("Learned Bias:", model.bias)
print("Predicted Labels:", y_pred)


</code>
  <div>
    
************************* EXP 5 () ************************
  </div>
<code>
  
from sklearn.datasets import make_classification
from sklearn import tree
from sklearn.model_selection import train_test_split
X, t = make_classification(100, 5, n_classes=2, shuffle=True, random_state=10)
X_train, X_test, t_train, t_test = train_test_split(
    X, t, test_size=0.3, shuffle=True, random_state=1)
model = tree.DecisionTreeClassifier()
model = model.fit(X_train, t_train)
predicted_value = model.predict(X_test)
print(predicted_value)
tree.plot_tree(model)
zeroes = 0
ones = 0
for i in range(0, len(t_train)):
    if t_train[i] == 0:
        zeroes += 1
    else:
        ones += 1
print(zeroes)
print(ones)
val = 1 - ((zeroes/70)(zeroes/70) + (ones/70)(ones/70))
print("Gini :", val)
match = 0
UnMatch = 0
for i in range(30):
    if predicted_value[i] == t_test[i]:
        match += 1
    else:
        UnMatch += 1
accuracy = match/30
print("Accuracy is: ", accuracy)

</code>
<div>
  
************************* EXP 6 (SVM) ************************
</div>
<code>
  
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

cancer = datasets.load_breast_cancer()
X = cancer.data
y = cancer.target
print(X)
print("\n")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state = 42)
svm_classifier = SVC(kernel = 'linear', C = 1.0)

svm_classifier.fit(X_train, y_train)
y_pred = svm_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print("\nClassification Report:\n", classification_report(y_test,
y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

</code>
<div>
  
************************* EXP 7 (Ensemble) ************************
</div>
<code>
  
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import VotingClassifier

# Load the dataset as an example
data = load_breast_cancer()
X, y = data.data, data.target
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)

# BAGGING (BootStrap Aggregating)

rf_classifier = RandomForestClassifier(n_estimators=100,random_state=42)
rf_classifier.fit(X_train, y_train)
rf_predictions = rf_classifier.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print("Predictions:", rf_predictions)
print(f"Accuracy: {rf_accuracy}\n")

# BOOSTING
# AdaBoost
adaboost_classifier = AdaBoostClassifier(n_estimators=100,
random_state=42)
adaboost_classifier.fit(X_train, y_train)
adaboost_predictions = adaboost_classifier.predict(X_test)
adaboost_accuracy = accuracy_score(y_test, adaboost_predictions)
print("Predictions:", adaboost_predictions)
print(f"Accuracy: {adaboost_accuracy}\n")
# Gradient Boost
gradientboost_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)
gradientboost_classifier.fit(X_train, y_train)
gradientboost_predictions = gradientboost_classifier.predict(X_test)
gradientboost_accuracy = accuracy_score(y_test,
gradientboost_predictions)
print("Predictions:", gradientboost_predictions)
print(f"Accuracy: {gradientboost_accuracy}\n")

# STACKING
# Stacking involves training multiple diverse models and then
# Create base models
base_models = [
('RandomForest', RandomForestClassifier(n_estimators=100,
random_state=42)),
('AdaBoost', AdaBoostClassifier(n_estimators=100,
random_state=42)),
('GradientBoosting',
GradientBoostingClassifier(n_estimators=100, random_state=42))
]
# Create a meta-classifier (Logistic Regression)
meta_classifier = LogisticRegression()
# Create the stacking ensemble
stacking_classifier = VotingClassifier(estimators=base_models,
voting='soft', n_jobs=-1)
stacking_classifier.fit(X_train, y_train)
stacking_predictions = stacking_classifier.predict(X_test)
stacking_accuracy = accuracy_score(y_test, stacking_predictions)
print("Predictions:", stacking_predictions)
print(f"Stacking Accuracy: {stacking_accuracy}")


</code>

<div>
  
************************* EXP 8 (PCA) ************************
</div>
<code>
  
import numpy as np
import matplotlib.pyplot as plt

# Create sample data
np.random.seed(0)
data = np.random.randn(2, 50)
data[1, :] = 2 * data[0, :] + 1 + 0.5 * np.random.randn(50)

# Standardize the data (mean-centering)
mean_centered_data = data - np.mean(data, axis=1).reshape(-1, 1)

# Compute the covariance matrix
cov_matrix = np.cov(mean_centered_data)

# Calculate eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Sort eigenvalues and eigenvectors
eigenvalue_index = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[eigenvalue_index]
eigenvectors = eigenvectors[:, eigenvalue_index]

# Choose the number of principal components
num_components = 1

# Project the data onto the principal component(s)
projected_data = np.dot(eigenvectors[:, :num_components].T, mean_centered_data)

# Display the covariance matrix
print("Covariance Matrix:")
print(cov_matrix)

# Display the eigenvalues
print("\nEigenvalues:")
print(eigenvalues)

# Display the eigenvectors
print("\nEigenvectors:")
print(eigenvectors)

# Display the first principal component
first_principal_component = eigenvectors[:, 0]
print("\nFirst Principal Component:")
print(first_principal_component)

# Plot the original data
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.scatter(data[0, :], data[1, :])
plt.title("Original Data")
plt.subplot(1, 3, 2)
plt.scatter(projected_data, np.zeros_like(projected_data))
plt.title("Data on First Principal Component")
plt.subplot(1, 3, 3)
plt.quiver(0, 0, first_principal_component[0], first_principal_component[1], angles='xy')
plt.title("First Principal Component")
</code>
  
</body>
</html>
